# Projeto de Sistemas Distribu√≠dos: Chat com Toler√¢ncia a Falhas

Este projeto √© um sistema de chat distribu√≠do e tolerante a falhas que implementa conceitos avan√ßados de sistemas distribu√≠dos. Ele utiliza uma arquitetura polyglot (Python, Go e JavaScript) e √© totalmente orquestrado com Docker Compose.

O sistema √© projetado para ser resiliente, com m√∫ltiplos servidores de back-end que se coordenam para eleger um l√≠der, sincronizar rel√≥gios e replicar dados, garantindo que n√£o haja perda de informa√ß√µes em caso de falha de um dos n√≥s.

## üöÄ Conceitos Implementados

* **Arquitetura Polyglot:** Os servi√ßos s√£o escritos na melhor linguagem para a tarefa:
    * **Python:** Backend (Servidor, Broker, Proxy, Refer√™ncia)
    * **Go:** Cliente interativo
    * **JavaScript (Node.js):** Cliente autom√°tico (bot)
* **Mensageria com ZeroMQ (ZMQ):** Utiliza os padr√µes REQ/REP (para comandos) e PUB/SUB (para broadcast de mensagens) para comunica√ß√£o desacoplada.
* **Orquestra√ß√£o:** Todos os servi√ßos s√£o gerenciados e conectados em rede pelo Docker Compose.
* **Rel√≥gios L√≥gicos (Lamport):** Todos os processos (clientes e servidores) mant√™m um rel√≥gio l√≥gico para garantir a ordem causal dos eventos.
* **Servi√ßo de Descoberta:** O servi√ßo `referencia` atua como um coordenador centralizado, rastreando servidores ativos atrav√©s de *heartbeats* e atribuindo *ranks*.
* **Elei√ß√£o de L√≠der (Bully Algorithm):** Os servidores detectam falhas do coordenador e iniciam uma elei√ß√£o (P2P) para selecionar um novo l√≠der com base no rank.
* **Sincroniza√ß√£o de Rel√≥gio (Christian's Algorithm):** Os servidores usam o l√≠der eleito para sincronizar seus rel√≥gios f√≠sicos periodicamente.
* **Replica√ß√£o de Dados Ativa:** Todas as opera√ß√µes de escrita s√£o replicadas para todos os servidores (via PUB/SUB) para garantir consist√™ncia e toler√¢ncia a falhas.

## ‚öôÔ∏è Vis√£o Geral dos Servi√ßos

| Servi√ßo | Linguagem | Dockerfile | Descri√ß√£o |
| :--- | :--- | :--- | :--- |
| `broker` | Python | `Dockerfile_broker` | **Broker REQ/REP.** Recebe comandos dos clientes e os balanceia (round-robin) entre os servidores. |
| `proxy` | Python | `Dockerfile_proxy` | **Broker PUB/SUB.** Recebe publica√ß√µes (mensagens de chat, replica√ß√£o) e as transmite para todos os inscritos. |
| `referencia` | Python | `Dockerfile_referencia` | **Servi√ßo de Descoberta.** Atribui ranks aos servidores, recebe *heartbeats* e fornece a lista de servidores ativos. |
| `servidor` | Python | `Dockerfile_servidor` | **Servidor de L√≥gica (R√©plicas: 3).** Processa a l√≥gica de neg√≥cio (login, etc.), participa da elei√ß√£o, sincroniza rel√≥gios e replica dados. |
| `cliente` | Go | `Dockerfile_cliente_go` | **Cliente Interativo.** Permite que um usu√°rio humano envie comandos (REQ) e receba mensagens (SUB). |
| `cliente_automatico` | JavaScript | `Dockerfile_cliente_automatico_js` | **Bot.** Cliente automatizado que faz login e envia mensagens em loop para gerar carga e testar a replica√ß√£o. |

## üöÄ Como Executar

O projeto √© totalmente conteinerizado. Voc√™ s√≥ precisa do Docker e Docker Compose instalados.

1.  **Construir e Iniciar todos os servi√ßos (em segundo plano):**
    ```bash
    docker compose up --build -d
    ```

2.  **Visualizar os logs (recomendado):**
    Para ver a elei√ß√£o, replica√ß√£o e sincronia acontecendo em tempo real.
    ```bash
    docker compose logs -f servidor referencia
    ```

3.  **Executar o Cliente Interativo (em Go):**
    Abra um novo terminal e use o `run` para iniciar o cliente interativo.
    ```bash
    docker compose run cliente
    ```

4.  **Escalar os Bots (Clientes Autom√°ticos):**
    Para simular m√∫ltiplos clientes, voc√™ pode escalar o `cliente_automatico`.
    ```bash
    docker compose up -d --scale cliente_automatico=5
    ```

5.  **Parar tudo:**
    ```bash
    docker compose down -v
    ```

---

## Parte 5: Consist√™ncia e Replica√ß√£o

### Problema

O `broker` (padr√£o REQ/REP) distribui as requisi√ß√µes de escrita (`login`, `channel`, `publish`, `msg`) entre as 3 r√©plicas do servidor usando *round-robin*. Isso resulta em cada servidor possuindo apenas uma fra√ß√£o do estado total (usu√°rios, canais) e do hist√≥rico de mensagens, levando √† perda de dados em caso de falha de um servidor.

### M√©todo de Implementa√ß√£o: Replica√ß√£o Ativa via PUB/SUB

Para resolver este problema, foi implementado um modelo de **Replica√ß√£o Ativa** com **Consist√™ncia Eventual**.

A escolha se deu por este m√©todo se integrar perfeitamente √† arquitetura de *proxy* PUB/SUB (XSUB/XPUB) j√° existente no projeto.

#### Troca de Mensagens para Replica√ß√£o

O fluxo de replica√ß√£o de dados funciona da seguinte maneira:

1.  **Requisi√ß√£o de Escrita:** Um cliente envia uma requisi√ß√£o de escrita (ex: `login`) ao `broker`.
2.  **Processamento Prim√°rio:** O `broker` encaminha a requisi√ß√£o para um servidor (ex: **Servidor 1**). A *thread principal* do Servidor 1 processa a requisi√ß√£o, salva a altera√ß√£o em seus arquivos locais (`users.json`, `channels.json` ou `messages.jsonl`) e envia a resposta de `OK` (REP) de volta ao cliente.
3.  **Publica√ß√£o (Broadcast):** Imediatamente ap√≥s o salvamento local, a *thread principal* do Servidor 1 tamb√©m **publica (PUB)** a requisi√ß√£o original completa em um t√≥pico interno chamado `replication` no *proxy* PUB/SUB.
4.  **Processamento da R√©plica (SUB):**
    * Todos os servidores (incluindo o Servidor 1) possuem uma *thread P2P* que est√° inscrita (SUB) no t√≥pico `replication`.
    * O *proxy* transmite a requisi√ß√£o para **todos** os servidores (Servidor 1, 2 e 3).
    * Ao receberem a mensagem no t√≥pico `replication`, as *threads P2P* de todos os servidores invocam a fun√ß√£o `handle_replication()`.
5.  **Escrita Replicada:** A fun√ß√£o `handle_replication()` executa a mesma l√≥gica de escrita do passo 2 (salva em `users.json`, `channels.json`, etc.).

#### Resultado

* **Toler√¢ncia a Falhas:** Todos os servidores agora possuem uma c√≥pia id√™ntica dos arquivos `users.json`, `channels.json` e `messages.jsonl`. Se um servidor falhar, nenhum dado √© perdido.
* **Consist√™ncia Eventual:** O cliente recebe uma resposta r√°pida (baixa lat√™ncia) do Servidor 1. Os Servidores 2 e 3 se tornam consistentes alguns milissegundos depois, quando recebem e processam a mensagem `replication`.
* **Idempot√™ncia:** O Servidor 1 (o originador) recebe sua pr√≥pria mensagem de replica√ß√£o e a processa uma segunda vez (uma vez na thread principal, outra na thread P2P). Isso √© intencional e seguro, pois as opera√ß√µes de escrita (salvar em dicion√°rio, adicionar em arquivo `.jsonl`) s√£o idempotentes ou seguras para repeti√ß√£o.
